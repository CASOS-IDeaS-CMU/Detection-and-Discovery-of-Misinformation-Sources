{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelStudio Evaluation Job Creation\n",
    "* Scrape articles using newspaper3k\n",
    "* Create articles summaries with spacy\n",
    "* Translate text of non-english articles\n",
    "* Output json struct compatible with label_studio_config.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keywords': ['trade',\n",
      "              'trend',\n",
      "              'animals',\n",
      "              'facebook',\n",
      "              'otter',\n",
      "              'animal',\n",
      "              'thailand',\n",
      "              'exotic',\n",
      "              'otters',\n",
      "              'fact',\n",
      "              'wild',\n",
      "              'feature',\n",
      "              'taking'],\n",
      " 'lang': 'en',\n",
      " 'summary': '(Editor’s note: Every now and then, The Adobo Chronicles gets '\n",
      "            'serious about issues regarding the destruction of our environment '\n",
      "            'or removing wild animals from their natural habitat.\\n'\n",
      "            'There are many promoters that can be found on social media '\n",
      "            'websites that are looking to sell exotic animals.\\n'\n",
      "            'From time immemorial there have been many exotic animals that '\n",
      "            'people have kept such as snakes, flying squirrels, Mexican '\n",
      "            'red-legged tarantulas, thus making the otter trade only the '\n",
      "            'latest trend.\\n'\n",
      "            'In Thailand, the trade of otters is commonplace and there are '\n",
      "            'many Facebook groups that exist for the same.\\n'\n",
      "            'There is no such legal process that is present that can aid in '\n",
      "            'preventing the trade of otters.',\n",
      " 'title': 'Feature: The Otter Side Of The Exotic Animal Trade Via Facebook',\n",
      " 'url': 'https://www.adobochronicles.com/2019/02/17/feature-the-otter-side-of-the-exotic-animal-trade-via-facebook/'}\n",
      "Feature: The Otter Side Of The Exotic Animal Trade Via Facebook\n",
      "UK: Meta receives final order to sell Giphy\n"
     ]
    }
   ],
   "source": [
    "import newspaper\n",
    "import pandas as pd\n",
    "from googletrans import Translator, constants\n",
    "from transformers import pipeline\n",
    "import pprint\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import json\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def summarize(text, per):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    select_length=int(len(sentence_tokens)*per)\n",
    "    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n",
    "    final_summary=[word.text for word in summary]\n",
    "    summary=''.join(final_summary)\n",
    "    return summary\n",
    "\n",
    "def get_article_info(a, url):\n",
    "\n",
    "    headline = a.title\n",
    "    keywords = None\n",
    "    summary = None\n",
    "    lang = None\n",
    "        \n",
    "    if headline:\n",
    "        try:\n",
    "            a.download()\n",
    "            a.parse()\n",
    "            a.nlp()\n",
    "        except:\n",
    "            print('summarize failed: ', url)\n",
    "        \n",
    "        keywords = a.keywords\n",
    "        summary = a.summary\n",
    "        \n",
    "        try:\n",
    "            lang = translator.detect(headline).lang\n",
    "            if  lang != 'en':\n",
    "                headline = translator.translate(headline.replace('\\n', '').replace('\\t', '')).text\n",
    "                keywords = [kw.text for kw in translator.translate(keywords)]\n",
    "                summary = translator.translate(summary).text\n",
    "        except:\n",
    "            print('translation failed: ', url)\n",
    "\n",
    "    return {\n",
    "        'url': url,\n",
    "        'title': headline,\n",
    "        'keywords': keywords,\n",
    "        'summary': summary,\n",
    "        'lang': lang\n",
    "    }\n",
    "\n",
    "# url = 'https://www.12minutos.com/63c955cfcb8f0/creativa-modular-en-el-top-3-de-constructoras-mas-importantes-del-mundo.html'\n",
    "url = 'https://www.adobochronicles.com/2019/02/17/feature-the-otter-side-of-the-exotic-animal-trade-via-facebook/'\n",
    "a = newspaper.Article(url)\n",
    "a.download()\n",
    "a.parse()\n",
    "pprint.pprint(get_article_info(a, a.url))\n",
    "print(a.title)\n",
    "print(Translator().translate(\"UK: Η Meta λαμβάνει την τελική εντολή να πουλήσει την Giphy\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buildling  100percentfedup.com\n",
      "Buildling  1010wins.radio.com\n",
      "Buildling  1011now.com\n",
      "Buildling  10tv.com\n",
      "Buildling  11alive.com\n"
     ]
    }
   ],
   "source": [
    "discovery_sample = pd.read_csv('../data/filtered_attrs.csv')\n",
    "discovery_articles = {}\n",
    "num_papers = 5\n",
    "for url in discovery_sample['url'][:num_papers]:\n",
    "    print('Buildling ', url)\n",
    "    paper = newspaper.build('http://'+url, memoize_articles=False)\n",
    "    discovery_articles[url] = paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100percentfedup.com\n",
      "1010wins.radio.com\n",
      "1011now.com\n",
      "10tv.com\n",
      "11alive.com\n"
     ]
    }
   ],
   "source": [
    "all_headlines = {}\n",
    "num_articles = 3\n",
    "\n",
    "for (url, paper) in discovery_articles.items():\n",
    "    print(url)\n",
    "    headlines = []\n",
    "    for a in paper.articles[:num_articles]:\n",
    "        headline = a.title\n",
    "        if headline and len(headline) > 10:\n",
    "            try:\n",
    "                a.download()\n",
    "                a.parse()\n",
    "                a.nlp()\n",
    "            except:\n",
    "                print('summarize failed: ', a.url)\n",
    "            headline = a.title\n",
    "            keywords = a.keywords\n",
    "            summary_newspaper = a.summary\n",
    "            try:\n",
    "                summary_spacy = summarize(a.text, 0.05)\n",
    "            except: \n",
    "                print('Spacy failed: ', a.url)\n",
    "                summary_spacy = \"\"\n",
    "            if translator.detect(headline).lang != 'en':\n",
    "                headline = translator.translate(headline.replace('\\n', '').replace('\\t', '')).text\n",
    "                keywords = [kw.text for kw in translator.translate(keywords)]\n",
    "                summary_newspaper = translator.translate(summary_newspaper).text\n",
    "                summary_spacy = translator.translate(summary_spacy).text\n",
    "            headlines.append((headline, a.url, a.top_image, keywords, summary_newspaper, summary_spacy))\n",
    "    if len(headlines) > 0:\n",
    "        all_headlines[url] = headlines\n",
    "\n",
    "json_format = []\n",
    "for (url, articles) in all_headlines.items():\n",
    "    for (headline, link, images, keywords, summary_newspaper, summary_spacy) in articles:\n",
    "        keywords = ', '.join([kw for kw in keywords if len(kw) > 4])\n",
    "        entry = {'headline': headline, 'link': link,'image': images,'keywords': keywords,'summary_1':summary_newspaper, 'summary_2': summary_spacy,'base_link':url}\n",
    "        json_format.append({'data':entry})\n",
    "\n",
    "with open('sample_formatted.json', 'w') as fp:\n",
    "    json.dump(json_format, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from requests_html import AsyncHTMLSession \n",
    "\n",
    "bad_filetypes = ['.jpg', '.png']\n",
    "\n",
    "def is_article(url, domain):\n",
    "    not_file = all([url[-len(ex):] != ex for ex in bad_filetypes])\n",
    "    return not_file and url.count('-') > 2 and (domain in url)\n",
    "\n",
    "all_links = []\n",
    "domain = 'foxnews.com/'\n",
    "oururl= urllib.request.urlopen('https://www.' + domain).read()\n",
    "soup = BeautifulSoup(oururl)\n",
    "html = lxml.html.document_fromstring(str(soup))\n",
    "html.make_links_absolute('https://www.' + domain)\n",
    "for element, attribute, url, pos in html.iterlinks():\n",
    "    if is_article(url, domain):\n",
    "        all_links.append(url)\n",
    "\n",
    "session = AsyncHTMLSession()\n",
    "async_l = []\n",
    "result = await session.get('https://www.' + domain)\n",
    "await result.html.arender()\n",
    "for url in result.html.absolute_links:\n",
    "    if is_article(url, domain):\n",
    "        all_links.append(url)\n",
    "\n",
    "all_links = list(set(all_links))\n",
    "len(all_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
